# Databricks notebook source
# MAGIC %pip install mlflow

# COMMAND ----------

# MAGIC %md 
# MAGIC TODO -- 
# MAGIC verify sample count per day?
# MAGIC convert to DLT
# MAGIC join OT data for Hitt?
# MAGIC Build a friggin dashboard
# MAGIC convert metadata to json to include in Repo
# MAGIC clean up PII???
# MAGIC 
# MAGIC 
# MAGIC 
# MAGIC PythonReference
# MAGIC 
# MAGIC https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-python-ref.html

# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC # Simplify ETL with Delta Live Table
# MAGIC 
# MAGIC DLT makes Data Engineering accessible for all. Just declare your transformations in SQL or Python, and DLT will handle the Data Engineering complexity for you.
# MAGIC 
# MAGIC <img style="float:right" src="https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.databricks.com%2Fdiscover%2Fpages%2Fgetting-started-with-delta-live-tables&psig=AOvVaw3yR74PCE0neSWnA9sBq814&ust=1673458952744000&source=images&cd=vfe&ved=0CA8QjRxqFwoTCLiWkMnGvfwCFQAAAAAdAAAAABAD" width="700"/>
# MAGIC 
# MAGIC **Accelerate ETL development** <br/>
# MAGIC Enable analysts and data engineers to innovate rapidly with simple pipeline development and maintenance 
# MAGIC 
# MAGIC **Remove operational complexity** <br/>
# MAGIC By automating complex administrative tasks and gaining broader visibility into pipeline operations
# MAGIC 
# MAGIC **Trust your data** <br/>
# MAGIC With built-in quality controls and quality monitoring to ensure accurate and useful BI, Data Science, and ML 
# MAGIC 
# MAGIC **Simplify batch and streaming** <br/>
# MAGIC With self-optimization and auto-scaling data pipelines for batch or streaming processing 
# MAGIC 
# MAGIC ## Our Delta Live Table pipeline
# MAGIC 
# MAGIC We'll be using as input a raw generated dataset containing information on our customers Loan and historical transactions. 
# MAGIC 
# MAGIC Our goal is to ingest this data in near real time and build table for our Analyst team while ensuring data quality.

# COMMAND ----------

# MAGIC %md-sandbox 
# MAGIC 
# MAGIC ## Bronze layer: incrementally ingest data leveraging Databricks Autoloader
# MAGIC 
# MAGIC <img style="float: right; padding-left: 10px" src="https://raw.githubusercontent.com/morganmazouchi/Tech-Summit-2022/main/InputData%20%26%20ML%20model/Techsummit%20DLT%202022%20-%20autoloader.png" width="800"/>
# MAGIC 
# MAGIC Our raw data is being sent to a cloud storage. 
# MAGIC 
# MAGIC Autoloader simplify this ingestion, including schema inference, schema evolution while being able to scale to millions of incoming files. 
# MAGIC 
# MAGIC Autoloader is available in SQL using the `cloud_files` function and can be used with a variety of format (json, csv, avro...):
# MAGIC 
# MAGIC 
# MAGIC #### STREAMING LIVE TABLE 
# MAGIC Defining tables as `STREAMING` will guarantee that you only consume new incoming data. 
# MAGIC 1. Compute results over append-only streams such as Kafka, Kinesis, or Autoloader (files on cloud storage)
# MAGIC 
# MAGIC 2. Delta tables with delta.appendOnly=true
# MAGIC 
# MAGIC 3. Produce results on demand: 
# MAGIC 
# MAGIC   - Lower latency: more frequent less data processing
# MAGIC   - Lower costs: by avoiding redundant reprocessing
# MAGIC     
# MAGIC     
# MAGIC #### Data sources:
# MAGIC 
# MAGIC During this hands on lab, we are using 2 datasources. 
# MAGIC One is a databricks dataset stored in  "/databricks-datasets/lending-club-loan-stats/LoanStats_*", while the second one is generated by faker library, and can be found in "/home/techsummit/dlt". Please note you only need to pass these two locations to the configuration of your pipeline when creating your pipeline. Run command 3, you can safely use the JSON configuration output for your pipeline creation.

# COMMAND ----------

#setup Environment and Libraries

import dlt
import mlflow
import datetime
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window
from pyspark.sql import Row

# COMMAND ----------

@dlt.table(
  name="kCal_union",
  comment="union All energy Silver tables"
)

@dlt.expect_or_drop("start_time_valid", "timestamp IS NOT NULL")
 
def workout_union():
  workoutUnion=[]
  workoutList=(spark
              .table("jmr_dlt.metadata_xml")
              .filter(col("table_name").contains("EnergyBurned"))
              .select(col('table_name'))
              .rdd.map(lambda x : x[0]).collect()
             )
  for t in workoutList:
    workoutUnion.append(f"{t}")
    
    
  target_tables = [dlt.read(t) for t in workoutUnion]
  unioned = functools.reduce(lambda x,y: x.union(y), target_tables)
  return (
    unioned.withColumn("kcal",col('value')).select(col("*"))
  )

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC ## Silver layer: joining tables while ensuring data quality
# MAGIC 
# MAGIC <img style="float:right"  src="https://raw.githubusercontent.com/morganmazouchi/Tech-Summit-2022/main/InputData%20%26%20ML%20model/Techsummit%20DLT%202022%20-%20Silver%20Layer.png" width="500"/>
# MAGIC 
# MAGIC Once the bronze layer is defined, we'll create the sliver layers by Joining data. Note that bronze tables are referenced using the `LIVE` namespace. 
# MAGIC 
# MAGIC To consume only increment from the Bronze layer like `raw_txs`, we'll be using the `stream` keyword: `stream(LIVE.raw_txs)`
# MAGIC 
# MAGIC Note that we don't have to worry about compactions, DLT handles that for us.
# MAGIC 
# MAGIC #### Expectations
# MAGIC 
# MAGIC DLT currently supports three modes for expectations:
# MAGIC 
# MAGIC | Mode | Behavior |
# MAGIC | ---- | -------- |
# MAGIC | `EXPECT(criteria)` in SQL or `@dlt.expect` in Python  | Record metrics for percentage of records that violate expectation <br> (**NOTE**: this metric is reported for all execution modes) |
# MAGIC | `EXPECT (criteria) ON VIOLATION FAIL UPDATE` in SQL or `@dlt.expect_or_fail` in Python| Fail the pipeline when expectation is not met |
# MAGIC | `EXPECT (criteria) ON VIOLATION DROP ROW` in SQL or `@dlt.expect_or_drop` in Python| Only process records that fulfill expectations |
# MAGIC 
# MAGIC By defining expectations (`CONSTRAINT <name> EXPECT <condition>`), you can enforce and track your data quality. See the [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-expectations.html) for more details

# COMMAND ----------

# goldHR dailyAgg

@dlt.table(name='dailyAgg_hr_gold',comment='avg, min, max HR by day')
@dlt.expect_or_drop("valid HR", "avgBpm IS NOT NULL")

def dailyAgg_hr_gold():
  return(dlt.read(name="heartrate").withColumn('bpm',col('value')).groupBy(col('dateDay')).agg(round(min(col('bpm')),2).alias('restingBpm'),round(avg(col('bpm')),2).alias('avgBpm'),round(max(col('bpm')),2).alias('maxBpm')))
# display(spark.read.table("jmr_ahk.heartrate").groupBy(col('dateDay')).agg(round(min(col('bpm')),2).alias('restingBpm'),round(avg(col('bpm')),2).alias('avgBpm'),round(max(col('bpm')),2).alias('maxBpm')))

# COMMAND ----------

# silver heartRate by Minute and Zone,bpmRange -- to be refined with ML model OR 

@dlt.table(name='MinuteAgg_HRzone_gold',comment='calc HR zone by minute')
@dlt.expect_or_drop("valid HR", "avgBpm IS NOT NULL")

def MinuteAgg_HRzone_gold():
  
  return(dlt
      .read(name='heartrate').withColumn('bpm',col('value'))
#       .withColumn('dateHour',to_timestamp(date_trunc("hour",col('timestamp')),'MM-dd-yyyy HH:mm'))    
#       .withColumn("dateMinute",to_timestamp(date_format(col("timestamp"), "MM-dd-yyyy HH:mm"),'MM-dd-yyyy HH:mm'))
#       .select(col('*'))
      .groupBy(col('dateMinute')).agg(round(min(col('bpm')),2).alias('minBpm'),round(avg(col('bpm')),2).alias('avgBpm'),round(max(col('bpm')),2).alias('maxBpm'))
      .orderBy(col('dateMinute').desc())).withColumn('hrZone',when(col('avgBpm')<=lit(132),1)
                                                     .when((col('avgBpm')>=lit(132)) & (col('avgBpm')<=lit(143)) ,2)
                                                     .when((col('avgBpm')>=lit(143)) & (col('avgBpm')<=lit(155)) ,3)
                                                     .when((col('avgBpm')>=lit(155)) & (col('avgBpm')<=lit(166)) ,4)
                                                     .when((col('avgBpm')>=lit(166)),5)
                                                     .otherwise(lit('0'))).withColumn('bpmRange',col('maxBpm')-col('minBpm'))

# COMMAND ----------

# # # gold workout dailyAgg with HR info - maybe add calorie and HR data?

# # sec2time= udf(lambda x: str(datetime.timedelta(seconds=x)))

# @dlt.table(name='dailyAgg_hr_workout_gold',comment='combination of HR and Workout daily')
# @dlt.expect_or_drop("valid day", "dateDay IS NOT NULL")

# def dailyAgg_hr_workout_gold():

  

#   # joinHR timestamp ranges info on workout 
#   sec2time= udf(lambda x: str(datetime.timedelta(seconds=x)))
#   hr=dlt.read(name="heartrate").withColumn('bpm',col('value')).select(col('timestamp'),col('bpm'))
  
#   workout=(dlt
#            .read(name="workout_union")
#            .withColumn("endtime",to_timestamp(date_format((col("timestamp").cast("long")+col('duration')).cast("timestamp"),'yyyy-MM-dd HH:mm:ss')))
#           )



#   joined_df = (workout
#                .join(hr)
#                .filter(workout.timestamp < hr.timestamp)
#                .filter(hr.timestamp < workout.endtime)
#                .groupBy(workout.dateDay)
#                .agg(round(min(col('bpm')),2).alias('restingBpm'),round(avg(col('bpm')),2).alias('avgBpm'),round(max(col('bpm')),2).alias('maxBpm'))
#               )
  
#   return(workout.join(joined_df,"dateDay")
#           .groupBy('dateDay')
#           .agg(
#             collect_set(col('activity_type')).alias('workoutTypes'),
#             count(col('uuid')).alias('exerciseCount'),
#             avg(col('avg_speed_mph')).alias("avgMPH"),
#             avg(col('energy_burned_kcal')).alias('avgKcal'),
#             sum(col('duration')).alias('exerciseTime').alias('totalExerciseTimeSeconds'),
#             sec2time(sum(col('duration')).alias('exerciseTime')).alias('totalExerciseTime'),
#             sum(col('distance_mile')).alias("sumMiles"),
#             sum(col('energy_burned_kcal')).alias('sumKcal'),
#             round(max(col('restingBpm')),2).alias('lowerWorkoutBpm'),
#             round(max(col('avgBpm')),2).alias('avgWorkoutBpm'),
#             round(max(col('maxBpm')),2).alias('maxWorkoutBpm'))
#           )


# COMMAND ----------

####FILL IN missing dates via manufactured table and left join -- yay
# will be superUnion with all other gold tables

@dlt.table(name='dailyAgg_mass_move_gold',comment='daily_mass_move_gold')
@dlt.expect_or_drop("valid HR", "avgBpm IS NOT NULL")

def daily_mass_move_gold():



  hr=(dlt.read(name="heartrate").withColumn('bpm',col('value'))
#       .withColumn("dateDay", to_date(col('dateDay'),'yyyy-MM-dd'))
      .groupBy(col('dateDay'))
      .agg(round(min(col('bpm')),2).alias('restingBpm'),round(avg(col('bpm')),2).alias('avgBpm'),round(max(col('bpm')),2).alias('maxBpm'))
     )

  move=(dlt.read(name='ActiveEnergyBurned')
#         .withColumn("dateDay", to_date(col('dateDay'),'yyyy-MM-dd'))
        .select(col('dateDay'),col('value'))).groupBy(col('dateDay')).agg(sum(col('value')).alias("dailyKcal"))
  
  
  mass=(dlt.read(name='bodymass').withColumn('weightlb',col('value')).withColumn("dateDay", to_date(col('dateDay'),'yyyy-MM-dd'))).groupBy(col('dateDay')).agg(avg(col('weightlb')).alias("weightlb"))
  
  



  


  # date_df.show(4)
  # df.show(4)
  joined = (hr.join(move,"dateDay","left")
                  .join(mass,"dateDay","left"))


  # rolling avg of last weightlb for days i missed weighing in

  w = (Window.orderBy(col("dateDay")).rowsBetween(-7, 7))
  
  return(joined.withColumn('weightlbRolling', avg("weightlb").over(w)).dropna(subset=['avgBpm','dailyKcal']))

# COMMAND ----------

# # goldHR dailyAgg

# @dlt.table(name='dataType_missing_gold',comment='avg, min, max HR by day')
# # @dlt.expect_or_drop("valid HR", "avgBpm IS NOT NULL")

# def dataType_missing_gold():
  
#   metadata=spark.table("jmr_dlt.metadata_xml")
#   datatype=dlt.read(name="datatype")
#   new=metadata.join(datatype, metadata.source_name==datatype.type,"right").filter(col('source_name').isNull()).withColumn("source_name",lit('new_event')).select(col('source_name'),col('type'))
#   return(new)

# COMMAND ----------

# @dlt.table(name='hrZoneSum',comment='rollingLbPredicitons using ARIMA model trained with AutoML')
# @dlt.expect_or_drop("valid date", "dateMinute IS NOT NULL")

# def hrZoneSum():
#   return(dlt.read(name="minuteagg_hrzone_gold").spark.sql("""SELECT count(*) as zoneMinutes, hrZone, DATE_TRUNC('day', dateMinute) as day 
#   FROM minuteagg_hrzone_gold 
#   GROUP BY hrZone, day 
#   ORDER BY day desc, hrZone asc"""))

# COMMAND ----------

# %python
# import mlflow
# from pyspark.sql.functions import struct, col


# logged_model = 'runs:/484133b5bdfc412693363f6efbe93454/model'

# # Load model as a Spark UDF. Override result_type if the model does not return double values.
# loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=logged_model, result_type='double')

# spark.udf.register("predict_lbs", loaded_model)

# COMMAND ----------



# model_uri = f"models:/jmr_health_lb/1"

# create spark user-defined function for model prediction.
# Note: : Here we use virtualenv to restore the python environment that was used to train the model.
# predict = mlflow.pyfunc.spark_udf(spark, model_uri, result_type="double")


# # Predict on a Spark DataFrame.
# run_id= "mlflow_run_id"
# model_name = "the_model_name_in_run"
model_uri = f"models:/jmr_health_lb/1"
# model_uri = "runs:/{run_id}/{model_name}".format(run_id=run_id, model_name=model_name)
loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=model_uri)

spark.udf.register("predict_lbs", loaded_model)


@dlt.table(name='rollingLbPredicitons',comment='rollingLbPredicitons using ARIMA model trained with AutoML')
@dlt.expect_or_drop("valid date", "dateDay IS NOT NULL")

def rollingLbPredicitons():
  return(dlt.read(name="dailyAgg_mass_move_gold").withColumn('predictions', predict_lbs(col('dateDay')))
     )
  
  

# COMMAND ----------

# def get_date_df():
#    initial_date = datetime.date(2022, 1, 1)
#    days = 720
#    one_day = datetime.timedelta(days=1)
#    all_days = [{"dateDay": initial_date + i * one_day} for i in range(days)]
#    return spark.createDataFrame(Row(**x) for x in all_days)

#######################

# https://stackoverflow.com/questions/42411184/filling-gaps-in-timeseries-spark
# from pyspark.sql.functions import col, min as min_, max as max_

# step = 15 * 60

# minp, maxp = df.select(
#     min_("periodstart").cast("long"), max_("periodstart").cast("long")
# ).first()

# reference = spark.range(
#     (minp / step) * step, ((maxp / step) + 1) * step, step
# ).select(col("id").cast("timestamp").alias("periodstart"))

# reference.join(df, ["periodstart"], "leftouter")

# COMMAND ----------

# MAGIC %md ## Tracking data quality
# MAGIC 
# MAGIC Expectations stats are automatically available as system table.
# MAGIC 
# MAGIC This information let you monitor your data ingestion quality. 
# MAGIC 
# MAGIC You can leverage DBSQL to request these table and build custom alerts based on the metrics your business is tracking.
# MAGIC 
# MAGIC 
# MAGIC See [how to access your DLT metrics]($./03-Log-Analysis)
# MAGIC 
# MAGIC <img width="500" src="https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/retail-dlt-data-quality-dashboard.png">
# MAGIC 
# MAGIC <a href="https://e2-demo-field-eng.cloud.databricks.com/sql/dashboards/6f73dd1b-17b1-49d0-9a11-b3772a2c3357-dlt---retail-data-quality-stats?o=1444828305810485" target="_blank">Data Quality Dashboard example</a>
